# Hands-On Exercise: Import Data from MySQL Using Apache Sqoop

GITHUB:
Create a new repository in your github and name it data_ingest. Create a folder and name it Sqooq.
Provide a markup with answers to the following.
Please provide screenshots of the following.

## 1. From the accounts table, import only the primary key, along with the first name, last name to HDFS directory /loudacre/accounts/user_info. Please save the file in text format with tab# delimiters.
Hint: You will have to figure out what the name of the table columns are in order to complete
this exercise.
> Primary key  알아내기: sqoop eval 명령어, SQL의 describe [table name] 사용
>> sqoop eval \
--connect jdbc:mysql://localhost/loudacre \
--username training --password training \
--query 'DESCRIBE accounts'

> text format with tab delimiters: --fields-terminated-by "\t" 옵션 사용

>> sqoop import \
--table accounts \
--connect jdbc:mysql://localhost/loudacre \
--username training --password training \
--target-dir /loudacre/accounts/user_info \
--columns "acct_num, first_name, last_name" \
--fields-terminated-by "\t"


## 2. This time save the same in parquet format with snappy compression. Save it in /loudacre/accounts/user_compressed. Provide.a screenshot of HUE with the new directory created.
> parquet format: --as-parquetfile 옵션 사용

> snappy compression: --compression-codec org.apache.hadoop.io.compress.SnappyCodec 사용

>> sqoop import \
--table accounts \
--connect jdbc:mysql://localhost/loudacre \
--username training --password training \
--target-dir /loudacre/accounts/user_compressed \
--columns "acct_num, first_name, last_name" \
--fields-terminated-by "\t" \
--as-parquetfile \
--compression-codec org.apache.hadoop.io.compress.SnappyCodec


## 3. Finally save in /loudacre/accounts/CA only clients whose state is from California. Save the file in avro format and compressed using snappy. From the terminal, display some of the records that you just imported. Take a screenshot and save it as CA_only.




